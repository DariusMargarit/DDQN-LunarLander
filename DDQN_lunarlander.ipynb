{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X82PfVEP0kRW"
   },
   "outputs": [],
   "source": [
    "!apt install swig\n",
    "!pip install gymnasium\n",
    "!pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def moving_average(data, window):\n",
    "    series = pd.Series(data)\n",
    "    return series.rolling(window).mean()\n",
    "\n",
    "\n",
    "def plot_rewards(values, path):\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(values)\n",
    "    plt.plot(moving_average(values, 100))\n",
    "    plt.savefig(path)\n",
    "\n",
    "\n",
    "def plot_epsilon(values, path):\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Epsilon')\n",
    "    plt.plot(values)\n",
    "    plt.savefig(path)\n",
    "\n",
    "\n",
    "def plot_steps_per_episode(values, path):\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Steps per episode')\n",
    "    plt.plot(values)\n",
    "    plt.savefig(path)\n",
    "\n",
    "\n",
    "def plot_fuel_consumption(values, path):\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Fuel consumption')\n",
    "    plt.plot(values)\n",
    "    plt.savefig(path)\n",
    "\n",
    "\n",
    "def plot_data():\n",
    "    with open(\"rewards.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        rewards = [float(line.strip()) for line in lines]\n",
    "        plot_rewards(rewards, \"rewards.png\")\n",
    "\n",
    "    with open(\"epsilons.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        epsilons = [float(line.strip()) for line in lines]\n",
    "        plot_epsilon(epsilons, \"epsilons.png\")\n",
    "\n",
    "    with open(\"steps_per_episode.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        steps_per_episode = [float(line.strip()) for line in lines]\n",
    "        plot_steps_per_episode(steps_per_episode, \"steps_per_episode.png\")\n",
    "\n",
    "    with open(\"fuel_consumption.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        fuel_consumption = [float(line.strip()) for line in lines]\n",
    "        plot_fuel_consumption(fuel_consumption, \"fuel_consumption.png\")\n",
    "\n",
    "\n",
    "def plot_eval_data():\n",
    "    with open(\"rewards.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        rewards = [float(line.strip()) for line in lines]\n",
    "        plot_rewards(rewards, \"rewards.png\")\n",
    "\n",
    "    with open(\"steps_per_episode.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        steps_per_episode = [float(line.strip()) for line in lines]\n",
    "        plot_steps_per_episode(steps_per_episode, \"steps_per_episode.png\")\n",
    "\n",
    "    with open(\"fuel_consumption.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        fuel_consumption = [float(line.strip()) for line in lines]\n",
    "        plot_fuel_consumption(fuel_consumption, \"fuel_consumption.png\")\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    \"episodes\": 1000,\n",
    "    \"max_steps\": 1000,\n",
    "    \"epsilon\": 1,\n",
    "    \"epsilon_decay\": 0.99,\n",
    "    \"epsilon_min\": 0.01,\n",
    "    \"update_freq\": 4,\n",
    "    \"gamma\": 0.999,\n",
    "    \"seed\": 12,\n",
    "    \"lr\": 0.0005,\n",
    "    \"tau\": 0.001,\n",
    "    \"buffer_size\": 100_000,\n",
    "    \"batch_size\": 64,\n",
    "    \"reward_target_mean\": 2000,\n",
    "    \"render\": False\n",
    "}\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        random.seed(parameters[\"seed\"])\n",
    "\n",
    "    def save(self, *args):\n",
    "        \"\"\"Saves a transition\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"Deep Q Network.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.seed = torch.manual_seed(parameters[\"seed\"])\n",
    "\n",
    "        self.fc1 = nn.Linear(state_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 32)\n",
    "        self.fc5 = nn.Linear(32, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return self.fc5(x)\n",
    "\n",
    "\n",
    "class Agent():\n",
    "\n",
    "    def __init__(self, env_name):\n",
    "        if parameters[\"render\"]:\n",
    "            self.env = gym.make(env_name, render_mode=\"human\")\n",
    "        else:\n",
    "            self.env = gym.make(env_name)\n",
    "        random.seed(parameters[\"seed\"])\n",
    "        state_size = self.env.observation_space.shape[0]\n",
    "        action_size = self.env.action_space.n\n",
    "\n",
    "        self.policy_net = DQN(state_size, action_size)\n",
    "        self.target_net = DQN(state_size, action_size)\n",
    "\n",
    "        # self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=parameters[\"lr\"])\n",
    "        # or self.optimizer = optim.RMSprop(self.policy_net.parameters())\n",
    "        self.memory = ReplayMemory(parameters[\"buffer_size\"])\n",
    "        self.steps = 0\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        self.policy_net.eval()\n",
    "        self.policy_net.train()\n",
    "\n",
    "        # epsilon-greedy\n",
    "        if random.random() > epsilon:\n",
    "            with torch.no_grad():\n",
    "                # return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "                return np.argmax(self.policy_net(state).cpu().data.numpy())\n",
    "        else:\n",
    "            return self.env.action_space.sample()\n",
    "\n",
    "    def learn(self, transitions, gamma):\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        state_batch = torch.from_numpy(\n",
    "            np.vstack([x for x in batch.state if x is not None])).float()\n",
    "        action_batch = torch.from_numpy(\n",
    "            np.vstack([x for x in batch.action if x is not None])).long()\n",
    "        reward_batch = torch.from_numpy(\n",
    "            np.vstack([x for x in batch.reward if x is not None])).float()\n",
    "        next_state_batch = torch.from_numpy(\n",
    "            np.vstack([x for x in batch.next_state if x is not None])).float()\n",
    "        done_batch = torch.from_numpy(np.vstack(\n",
    "            [x for x in batch.done if x is not None]).astype(np.uint8)).float()\n",
    "\n",
    "        Q_argmax = self.policy_net(next_state_batch).detach()\n",
    "        _, a_max = Q_argmax.max(1)\n",
    "\n",
    "        Q_target_next = self.target_net(\n",
    "            next_state_batch).detach().gather(1, a_max.unsqueeze(1))\n",
    "\n",
    "        Q_target = reward_batch + \\\n",
    "                   (gamma * Q_target_next * (1 - done_batch))\n",
    "\n",
    "        Q_expected = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # loss = F.mse_loss(Q_expected, Q_target)\n",
    "\n",
    "        # or use Huber loss\n",
    "        loss = F.smooth_l1_loss(Q_expected, Q_target)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update target network\n",
    "        for target_param, policy_param in zip(self.target_net.parameters(), self.policy_net.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                parameters[\"tau\"] * policy_param.data + (1. - parameters[\"tau\"]) * target_param.data)\n",
    "\n",
    "    def train(self):\n",
    "        reward_history = []\n",
    "        steps_per_episode = []\n",
    "        epsilons = []\n",
    "        fuel_consumption = []\n",
    "        rolling_reward_history = deque(maxlen=100)\n",
    "        epsilon = parameters[\"epsilon\"]\n",
    "        for i_episode in range(parameters[\"episodes\"]):\n",
    "            state = self.env.reset()[0]\n",
    "            reward, fuel = 0, 0\n",
    "            steps = parameters[\"max_steps\"]\n",
    "            for t in range(parameters[\"max_steps\"]):\n",
    "                action = self.act(state, epsilon)\n",
    "                if action == 2:\n",
    "                    fuel += 1\n",
    "                elif action != 0:\n",
    "                    fuel += 0.1\n",
    "                next_state, r, done, _, _ = self.env.step(action)\n",
    "                self.memory.save(state, action, next_state, r, done)\n",
    "\n",
    "                self.steps = (self.steps + 1) % parameters[\"update_freq\"]\n",
    "                if self.steps == 0:\n",
    "                    if len(self.memory) >= parameters[\"batch_size\"]:\n",
    "                        transitions = self.memory.sample(parameters[\"batch_size\"])\n",
    "                        self.learn(transitions, parameters[\"gamma\"])\n",
    "                state = next_state\n",
    "                reward += r\n",
    "                if done:\n",
    "                    steps = t\n",
    "                    break\n",
    "\n",
    "            reward_history.append(reward)\n",
    "            steps_per_episode.append(steps)\n",
    "            rolling_reward_history.append(reward)\n",
    "            epsilons.append(epsilon)\n",
    "            fuel_consumption.append(fuel)\n",
    "            epsilon = max(parameters[\"epsilon_decay\"] * epsilon, parameters[\"epsilon_min\"])\n",
    "\n",
    "            print('\\rEpisode {}\\tAverage Reward: {:.2f}'.format(\n",
    "                i_episode + 1, np.mean(rolling_reward_history)), end=\"\")\n",
    "            if (i_episode + 1) % 100 == 0:\n",
    "                print('\\rEpisode {}\\tAverage Reward: {:.2f}'.format(\n",
    "                    i_episode + 1, np.mean(rolling_reward_history)))\n",
    "                torch.save(self.policy_net.state_dict(), 'model.pth')\n",
    "            if np.mean(rolling_reward_history) >= parameters[\"reward_target_mean\"]:\n",
    "                print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(\n",
    "                    i_episode + 1 - 100, np.mean(rolling_reward_history)))\n",
    "                torch.save(self.policy_net.state_dict(), 'model.pth')\n",
    "                break\n",
    "\n",
    "        with open(\"rewards.txt\", \"w\") as f:\n",
    "            for elem in reward_history:\n",
    "                f.write(str(elem) + '\\n')\n",
    "        with open(\"epsilons.txt\", \"w\") as f:\n",
    "            for elem in epsilons:\n",
    "                f.write(str(elem) + '\\n')\n",
    "        with open(\"steps_per_episode.txt\", \"w\") as f:\n",
    "            for elem in steps_per_episode:\n",
    "                f.write(str(elem) + '\\n')\n",
    "        with open(\"fuel_consumption.txt\", \"w\") as f:\n",
    "            for elem in fuel_consumption:\n",
    "                f.write(str(elem) + '\\n')\n",
    "\n",
    "    def test(self):\n",
    "        self.policy_net.load_state_dict(torch.load(\n",
    "            'model.pth', map_location=lambda storage, loc: storage))\n",
    "\n",
    "        test_scores = []\n",
    "        steps_per_episode = []\n",
    "        fuel_consumption = []\n",
    "        for j in range(10000):\n",
    "            state = self.env.reset()[0]\n",
    "            reward, fuel = 0, 0\n",
    "            steps = parameters[\"max_steps\"]\n",
    "            for k in range(parameters[\"max_steps\"]):\n",
    "                action = self.act(state, epsilon=0)\n",
    "                if action == 2:\n",
    "                    fuel += 1\n",
    "                elif action != 0:\n",
    "                    fuel += 0.1\n",
    "                state, r, done, _, _ = self.env.step(action)\n",
    "                reward += r\n",
    "                if done:\n",
    "                    steps = k\n",
    "                    break\n",
    "\n",
    "            print('Episode {}: {}'.format(j + 1, reward))\n",
    "            test_scores.append(reward)\n",
    "            steps_per_episode.append(steps)\n",
    "            fuel_consumption.append(fuel)\n",
    "\n",
    "        avg_score = sum(test_scores) / len(test_scores)\n",
    "\n",
    "        print('\\rAverage reward: {:.2f}'.format(avg_score))\n",
    "\n",
    "        with open(\"rewards.txt\", \"w\") as f:\n",
    "            for elem in test_scores:\n",
    "                f.write(str(elem) + '\\n')\n",
    "        with open(\"steps_per_episode.txt\", \"w\") as f:\n",
    "            for elem in steps_per_episode:\n",
    "                f.write(str(elem) + '\\n')\n",
    "        with open(\"fuel_consumption.txt\", \"w\") as f:\n",
    "            for elem in fuel_consumption:\n",
    "                f.write(str(elem) + '\\n')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    agent = Agent('RewardCustomLunarLander')\n",
    "    # agent.train()\n",
    "    # plot_data()\n",
    "    agent.test()\n",
    "    plot_eval_data()\n"
   ],
   "metadata": {
    "id": "g21rRwkLMitH",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "b123b26b-0c82-4712-bc58-30eef175ac61"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
